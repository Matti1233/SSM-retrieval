# -*- coding: utf-8 -*-
"""Surface Soil Moisture Retrieval Slovenia

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Po7W2kfd8WDqjdO09h7_7VJlpp6BZPvo
"""

import ee
import geemap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Trigger the authentication flow.
ee.Authenticate()
# Initialize the library. (find the name in the GEE code editor under asset tab, written in Blue)
ee.Initialize(project='') # provide username

# Mount drive
from google.colab import drive
drive.mount('') # provide pathway to drive

# Define geemap.Map()
m = geemap.Map()
# Setup country border collection
borders = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017')
# Select slovenia from country border collection
Slovenia = borders.filterMetadata('country_na', 'equals', 'Slovenia')
AOI = ee.Geometry.BBox(14.4700, 46.0503, 14.4715, 46.0489)
#AOI = ee.Geometry.Point(14.471, 46.049)

border = ee.Image().byte().paint(featureCollection= AOI, color='red', width=2)
m.addLayer(border, {'palette': 'red'}, "border")

m.centerObject(AOI, 16)
m

# Load and filter Sentinel-1 dataset, see: https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S1_GRD
# filter the collection to only include images over slovenia
# select date range
# select type of polarisation (VV, VH, HH OR HV)
# select instument mode and resolution
sentinel_1 = (ee.ImageCollection('COPERNICUS/S1_GRD')
    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))
    .filter(ee.Filter.eq('instrumentMode', 'IW'))
    .filter(ee.Filter.eq('resolution_meters', 10))
)

m.addLayer(sentinel_1.select('VV').filterDate('2024-01-01', '2024-12-30').mean(), {'palette': ['white', 'black'], 'min': -20, 'max': 5}, 'sentinel')

# Load and filter SMAP dataset, see: https://developers.google.com/earth-engine/datasets/catalog/NASA_SMAP_SPL4SMGP_007
# filter for slovenia, select upper 5cm of soil, filter date
SMAP = (ee.ImageCollection("NASA/SMAP/SPL4SMGP/007")
    .select('sm_surface')
    .filterDate('2019-01-01', '2024-12-30')
    .map(lambda image: image.clip(AOI))
)

# load Modis NDVI data, see: https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD13Q1
ndvi = (ee.ImageCollection("MODIS/061/MOD13Q1").select('NDVI')
    .filterDate('2015-01-01', '2024-12-30')
)

# Load digital elevation model and calculate slope, see: https://gee-community-catalog.org/projects/fabdem/
FABDEM = ee.ImageCollection("projects/sat-io/open-datasets/FABDEM")
FABDEM_proj = FABDEM.first().projection();
FABDEM = FABDEM.mosaic().setDefaultProjection(FABDEM_proj).clip(Slovenia)
terrain_slope = ee.Terrain.slope(FABDEM)

# Landuse map, see: https://gee-community-catalog.org/projects/glc_fcs/

#Yearly data from 2000-2022
annual = ee.ImageCollection('projects/sat-io/open-datasets/GLC-FCS30D/annual') \
            .map(lambda image: image.clip(Slovenia))
#Five-Yearly data for 1985-90, 1990-95 and 1995-2000
fiveyear = ee.ImageCollection('projects/sat-io/open-datasets/GLC-FCS30D/five-years-map') \
              .map(lambda image: image.clip(Slovenia))

#Classification scheme has 36 classes (35 landcover classes and 1 fill value)
classValues = [10, 11, 12, 20, 51, 52, 61, 62, 71, 72, 81, 82, 91, 92, 120, 121, 122, 130, 140, 150, 152, 153, 181, 182, 183, 184, 185, 186, 187, 190, 200, 201, 202, 210, 220, 0];
classNames = ['Rainfed_cropland', 'Herbaceous_cover_cropland', 'Tree_or_shrub_cover_cropland', 'Irrigated_cropland', 'Open_evergreen_broadleaved_forest', 'Closed_evergreen_broadleaved_forest', 'Open_deciduous_broadleaved_forest', 'Closed_deciduous_broadleaved_forest', 'Open_evergreen_needle_leaved_forest', 'Closed_evergreen_needle_leaved_forest', 'Open_deciduous_needle_leaved_forest', 'Closed_deciduous_needle_leaved_forest', 'Open_mixed_leaf_forest', 'Closed_mixed_leaf_forest', 'Shrubland', 'Evergreen_shrubland', 'Deciduous_shrubland', 'Grassland', 'Lichens_and_mosses', 'Sparse_vegetation', 'Sparse_shrubland', 'Sparse_herbaceous', 'Swamp', 'Marsh', 'Flooded_flat', 'Saline', 'Mangrove', 'Salt_marsh', 'Tidal_flat', 'Impervious_surfaces', 'Bare_areas', 'Consolidated_bare_areas', 'Unconsolidated_bare_areas', 'Water_body', 'Permanent_ice_and_snow', 'Filled_value'];
classColors = ['#ffff64', '#ffff64', '#ffff00', '#aaf0f0', '#4c7300', '#006400', '#a8c800', '#00a000', '#005000', '#003c00', '#286400', '#285000', '#a0b432', '#788200', '#966400', '#964b00', '#966400', '#ffb432', '#ffdcd2', '#ffebaf', '#ffd278', '#ffebaf', '#00a884', '#73ffdf', '#9ebb3b', '#828282', '#f57ab6', '#66cdab', '#444f89', '#c31400', '#fff5d7', '#dcdcdc', '#fff5d7', '#0046c8', '#ffffff', '#ffffff'];

#Mosaic the data into a single image
annualMosaic = annual.mosaic();
fiveYearMosaic = fiveyear.mosaic();

#Rename bands from b1, b2, etc. to 2000, 2001, etc.
fiveYearsList = ee.List.sequence(1985, 1995, 5).map(lambda year: ee.Number(year).format('%04d'))
fiveyearMosaicRenamed = fiveYearMosaic.rename(fiveYearsList)
yearsList = ee.List.sequence(2000, 2022).map(lambda year: ee.Number(year).format('%04d'))
annualMosaicRenamed = annualMosaic.rename(yearsList)
years = fiveYearsList.cat(yearsList)

#Convert the multiband image to an ImageCollection
fiveYearlyMosaics = fiveYearsList.map(lambda year:
    fiveyearMosaicRenamed.select([year]).set({'system:time_start': \
        ee.Date.fromYMD(ee.Number.parse(year), 1, 1).millis(), \
       'system:index': year, 'year': ee.Number.parse(year)}))
yearlyMosaics = yearsList.map(lambda year:
    annualMosaicRenamed.select([year]).set({'system:time_start': \
        ee.Date.fromYMD(ee.Number.parse(year), 1, 1).millis(), \
       'system:index': year, 'year': ee.Number.parse(year)}))
allMosaics = fiveYearlyMosaics.cat(yearlyMosaics)
mosaicsCol = ee.ImageCollection.fromImages(allMosaics)

#Recode the class values, assigning 0 to urban,water and ice
newClassValues = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,0,1]
def renameClasses(image):
    reclassified = image.remap(classValues, newClassValues).rename('classification')
    return reclassified

# Masking backscatter based on slope and landcover

# Landcover mask
year = 2020
landcoverMask = mosaicsCol.map(renameClasses).filter(ee.Filter.eq('year', year)).first()
m.addLayer(landcoverMask, {}, 'landcoverMask')

# Slope mask
slopeMask = (ee.Image(1).clip(Slovenia)
    .where(terrain_slope.lte(20), 1)
    .where(terrain_slope.gt(20), 0)
)
m.addLayer(slopeMask, {}, 'slopeMask')

# Combined landcover & slope mask
def geography_mask(image):
  maskLayer1 = landcoverMask
  maskLayer2 = slopeMask
  doubleMask = landcoverMask.multiply(slopeMask)
  VV = image.select('VV').add(ee.Image(999))
  combined = doubleMask.multiply(VV).rename('masked_VV')
  masked_VV = combined.updateMask(combined.select('masked_VV').neq(0))
  masked_VV = masked_VV.select('masked_VV').subtract(ee.Image(999))
  return image.addBands(masked_VV)

doubleMask = landcoverMask.multiply(slopeMask)
m.addLayer(doubleMask, {}, 'doubleMask')

# Masking backscatter based on energy value
# create mask that removes all images from a collection where -20 < VV < -5
def energy_mask(image):
    VV = image.select('VV')
    masked_VV = VV.updateMask(VV.gt(-20) \
                   .And(VV.lt(-5))).rename('masked_VV')
    return image.addBands(masked_VV)

# Applying no mask
def no_mask(image):
    VV = image.select('VV')
    masked_VV = VV.rename('masked_VV')
    return image.addBands(masked_VV)

# Select one masking option:
S1_masked = sentinel_1.map(geography_mask) # Put a '#' in front of the 'S1_masked' you do not want to use
S1_masked = sentinel_1.map(energy_mask) # Put a '#' in front of the 'S1_masked' you do not want to use

S1_geo_masked = sentinel_1.map(geography_mask)
S1_ene_masked = sentinel_1.map(energy_mask)
S1_no_mask = sentinel_1.map(no_mask)

m.addLayer(S1_geo_masked.select('masked_VV').filterDate('2024-01-01', '2024-12-30').mean(), {'palette': ['white', 'black'], 'min': -20, 'max': 5}, 'sentinel_geographymask')
m.addLayer(S1_ene_masked.select('masked_VV').filterDate('2024-01-01', '2024-12-30').mean(), {'palette': ['white', 'black'], 'min': -20, 'max': 5}, 'sentinel_energymask')

# reproject backscatter values to larger cell size
reprojection_scale = 1000 # reprojection_scale value can be changed

# Create frame to which sentinel can be reprojected
frame = ndvi.reduce(ee.Reducer.mean()).rename('frame')
S1_masked = S1_masked.map(lambda image: image.addBands(frame))

S1_geo_masked = S1_geo_masked.map(lambda image: image.addBands(frame))
S1_ene_masked = S1_ene_masked.map(lambda image: image.addBands(frame))
S1_no_mask = S1_no_mask.map(lambda image: image.addBands(frame))

# Reproject function
# reprojects the sentinel-1 data (originally in 10m scale) to a new scale
# new scale is based on previously determined 'reprojection_scale'
def reproject(image):
  masked = image.select("masked_VV").add(ee.Image(999))
  frame = image.select('frame')

  imageReducers = masked.reduceResolution(reducer= ee.Reducer.mean().combine(
      reducer2= ee.Reducer.count(), sharedInputs= True), maxPixels= reprojection_scale*reprojection_scale/100) \
      .reproject(crs= frame.projection(), scale=reprojection_scale)

  mean = imageReducers.select(0).rename('mean')
  count = imageReducers.select(1).rename('count')

  removeHighNullCount = imageReducers.select(0).updateMask(imageReducers.select(1).gte(reprojection_scale/1.5))
  reprojected = removeHighNullCount.subtract(ee.Image(999)).rename("reprojected_VV")

  bands = [mean, count, reprojected]
  return image.addBands(bands)

S1_reprojected = S1_masked.map(reproject)

S1_ene_reprojected = S1_ene_masked.map(reproject)
m.addLayer(S1_ene_reprojected.select('reprojected_VV').filterDate('2024-01-01', '2024-12-30').mean(), {'palette': ['white', 'black'], 'min': -25, 'max': 10}, 'sentinel_ene_reproject')
S1_geo_reprojected = S1_geo_masked.map(reproject)
m.addLayer(S1_geo_reprojected.select('reprojected_VV').filterDate('2024-01-01', '2024-12-30').mean(), {'palette': ['white', 'black'], 'min': -25, 'max': 10}, 'sentinel_geo_reproject')
S1_no_mask_reprojected = S1_no_mask.map(reproject)
m.addLayer(S1_no_mask_reprojected.select('reprojected_VV').filterDate('2024-01-01', '2024-12-30').mean(), {'palette': ['white', 'black'], 'min': -25, 'max': 10}, 'sentinel_no_mask_reproject')

# Turn band data from image collection into pandas data frame

def ee_array_to_df(arr, list_of_bands):
    """Transforms client-side ee.Image.getRegion array to pandas.DataFrame."""
    df = pd.DataFrame(arr)

    # Rearrange the header.
    headers = df.iloc[0]
    df = pd.DataFrame(df.values[1:], columns=headers)

    # Remove rows without data inside.
    df = df.dropna(subset=[*list_of_bands])

    # Convert the data to numeric values.
    for band in list_of_bands:
        df[band] = pd.to_numeric(df[band], errors='coerce')

    # Convert the time field into a datetime.
    df['date'] = pd.to_datetime(df['time'], unit='ms')

    # Keep the columns of interest.
    df = df[['longitude', 'latitude', 'date', *list_of_bands]]

    return df

S1_reprojected.first().bandNames().getInfo()

ts1 = S1_reprojected.select('angle', 'reprojected_VV').getRegion(AOI, reprojection_scale/10).getInfo()
ts2 = S1_reprojected.select('masked_VV').getRegion(AOI, reprojection_scale/10).getInfo()

df1 = ee_array_to_df(ts1, ['angle', 'reprojected_VV'])
df2 = ee_array_to_df(ts2, ['masked_VV'])
df = df1.merge(df2)

df.sort_values(by='date')

# Adjusting backscatter based on incidence angle
def adjusted_monthly(months):
  df_adjusted_monthly = pd.DataFrame()

  for i in months:
    adjusted_month = df.loc[(df['date'].dt.strftime('%m') == i)]

    X = df['angle'].loc[(df['date'].dt.strftime('%m') == i)]
    Y = df['reprojected_VV'].loc[(df['date'].dt.strftime('%m') == i)]


    slope, intercept = np.polyfit(X, Y, 1)

    adjusted_month['adjusted_VV'] = [(x-slope*(y-40)) for x, y in zip(Y, X)]

    df_adjusted_monthly = pd.concat([df_adjusted_monthly, adjusted_month])
  globals()['df_adjusted_monthly'] = df_adjusted_monthly

adjusted_monthly(['01','02','03','04','05','06','07','08','09','10','11','12'])
df = df_adjusted_monthly.sort_values(by='date')

bottom_p = df['adjusted_VV'].quantile([0,.01,.02,.03,.04,.05]).mean()
top_p = df['adjusted_VV'].quantile([1,.99,.98,.97,.96,.95]).mean()


# define variables based on B. Bauer-Marschallinger et al., "Toward Global Soil Moisture Monitoring With Sentinel-1: Harnessing Assets and Overcoming Obstacles," doi: 10.1109/TGRS.2018.2858004
k = (95-5)/(top_p - bottom_p)
d = (95-(k*top_p))
bs_dry = ((0-d)/k)
bs_wet = ((100-d)/k)

df['SSM'] = ((df['adjusted_VV']-bs_dry)/(bs_wet-bs_dry))*(100)

df['SSM'] = df['SSM'][(df['SSM'] < 120)&(df['SSM'] > -20)]
df['SSM'] = df['SSM'].clip(upper=100, lower=0)

df

# Import in-field soil moisture data and turn into dataframe

df_crns = pd.read_excel('') # read file path
columns_titles = ['Date','SWC_CRNS','AVG_of_three_sensors_10cm']
df_crns = df_crns.reindex(columns=columns_titles)
df_crns.columns = ['date','SWC_CRNS','AVG_of_three_sensors_10cm']
df_crns = df_crns.loc[(df_crns['date'] < '2024-12-31')]
df_crns

def adjusted_monthly_SSM(months):
  df_adjusted_monthly = pd.DataFrame()
  # get min and max SSM values per month and their slope in correspondence with point sensor or crns
  for i in months:
    adjusted_month = df.loc[(df['date'].dt.strftime('%m') == i)]
    adjusted_month_crns = df_crns.loc[(df_crns['date'].dt.strftime('%m') == i)]

    adjusted_month['SSM_monthly_min'] = adjusted_month['SSM'].min()
    adjusted_month['SSM_monthly_max'] = adjusted_month['SSM'].max()
    adjusted_month['SSM_monthly_mean'] = adjusted_month['SSM'].mean()
    adjusted_month['sensor_slope_monthly'] = (df_crns['AVG_of_three_sensors_10cm'].max()-df_crns['AVG_of_three_sensors_10cm'].min()) \
                    /((adjusted_month['SSM_monthly_max']-adjusted_month['SSM_monthly_min']))
    adjusted_month['crns_slope_monthly'] = (df_crns['SWC_CRNS'].max()-df_crns['SWC_CRNS'].min()) \
                    /((adjusted_month['SSM_monthly_max']-adjusted_month['SSM_monthly_min']))


    #adjusted_month['SSM_monthly'] = ((X-top_p)/(bottom_p-top_p))*(100)

    df_adjusted_monthly = pd.concat([df_adjusted_monthly, adjusted_month])
  globals()['df_adjusted_monthly'] = df_adjusted_monthly

adjusted_monthly_SSM(['01','02','03','04','05','06','07','08','09','10','11','12'])

df = df_adjusted_monthly.sort_values(by='date')

df_filtered = df.loc[(df['date'] > pd.to_datetime(df_crns['date'].min()))&(df['date'] < pd.to_datetime(df_crns['date'].max()))]
df_crns = df_crns.loc[(df_crns['date'] < pd.to_datetime(df['date'].max()))]

# rescaling previously determined SSM data to in-field measurement range

slope_sensors = (df_crns['AVG_of_three_sensors_10cm'].max()-df_crns['AVG_of_three_sensors_10cm'].min())/(df['adjusted_VV'].max()-df['adjusted_VV'].min())
slope_sensors_filtered = (df_crns['AVG_of_three_sensors_10cm'].max()-df_crns['AVG_of_three_sensors_10cm'].min())/(df_filtered['adjusted_VV'].max()-df_filtered['adjusted_VV'].min())
slope_sensorsssm = (df_crns['AVG_of_three_sensors_10cm'].max()-df_crns['AVG_of_three_sensors_10cm'].min())/(df['SSM'].max()-df['SSM'].min())
slope_sensorsssm_filtered = (df_crns['AVG_of_three_sensors_10cm'].max()-df_crns['AVG_of_three_sensors_10cm'].min())/(df_filtered['SSM'].max()-df_filtered['SSM'].min())
slope_sensors_monthly = (df_crns['AVG_of_three_sensors_10cm'].max()-df_crns['AVG_of_three_sensors_10cm'].min())/(df['SSM_monthly'].max()-df['SSM_monthly'].min())
slope_sensors_monthly_filtered = (df_crns['AVG_of_three_sensors_10cm'].max()-df_crns['AVG_of_three_sensors_10cm'].min())/(df_filtered['SSM_monthly'].max()-df_filtered['SSM_monthly'].min())

slope_of_change = (df_crns['SWC_CRNS'].max()-df_crns['SWC_CRNS'].min())/(df['adjusted_VV'].max()-df['adjusted_VV'].min())
slope_of_change_filtered = (df_crns['SWC_CRNS'].max()-df_crns['SWC_CRNS'].min())/(df_filtered['adjusted_VV'].max()-df_filtered['adjusted_VV'].min())
slope_ssm = (df_crns['SWC_CRNS'].max()-df_crns['SWC_CRNS'].min())/(df['SSM'].max()-df['SSM'].min())
slope_ssm_filtered = (df_crns['SWC_CRNS'].max()-df_crns['SWC_CRNS'].min())/(df_filtered['SSM'].max()-df_filtered['SSM'].min())
slope_monthly = (df_crns['SWC_CRNS'].max()-df_crns['SWC_CRNS'].min())/(df['SSM_monthly'].max()-df['SSM_monthly'].min())
slope_monthly_filtered = (df_crns['SWC_CRNS'].max()-df_crns['SWC_CRNS'].min())/(df_filtered['SSM_monthly'].max()-df_filtered['SSM_monthly'].min())

component2 = df_crns['SWC_CRNS']
df_filtered['SSM2'] = (abs(df_filtered['adjusted_VV']-df_filtered['adjusted_VV'].min())*slope_of_change_filtered)+component2.min()
df_filtered['SSM22'] = (abs(df_filtered['adjusted_VV']-df_filtered['adjusted_VV'].min())*slope_of_change)+component2.min()
df_filtered['SSM23'] = (abs(df_filtered['SSM']-df_filtered['SSM'].min())*slope_ssm_filtered)+component2.min()
df_filtered['SSM24'] = (abs(df_filtered['SSM']-df_filtered['SSM'].min())*slope_ssm)+component2.min()

df_filtered['SSM210'] = [((abs(x-y)*slope_ssm)+component2.min()) for x, y in zip(df_filtered['SSM'], df_filtered['SSM_monthly_min'])]
df_filtered['SSM211'] = [((abs(x-y)*slope_ssm_filtered)+component2.min()) for x, y in zip(df_filtered['SSM'], df_filtered['SSM_monthly_min'])]
df_filtered['SSM215'] = [((abs(x-y)*z)+component2.min()) for x, y, z in zip(df_filtered['SSM'], df_filtered['SSM_monthly_min'], df_filtered['crns_slope_monthly'])]

component3 = df_crns['AVG_of_three_sensors_10cm']
df_filtered['SSM3'] = (abs(df_filtered['adjusted_VV']-df_filtered['adjusted_VV'].min())*slope_sensors_filtered)+component3.min()
df_filtered['SSM32'] = (abs(df_filtered['adjusted_VV']-df_filtered['adjusted_VV'].min())*slope_sensors)+component3.min()
df_filtered['SSM33'] = (abs(df_filtered['SSM']-df_filtered['SSM'].min())*slope_sensorsssm_filtered)+component3.min()
df_filtered['SSM34'] = (abs(df_filtered['SSM']-df_filtered['SSM'].min())*slope_sensorsssm)+component3.min()

df_filtered['SSM310'] = [((abs(x-y)*slope_sensorsssm)+component3.min()) for x, y in zip(df_filtered['SSM'], df_filtered['SSM_monthly_min'])]
df_filtered['SSM311'] = [((abs(x-y)*slope_sensorsssm_filtered)+component3.min()) for x, y in zip(df_filtered['SSM'], df_filtered['SSM_monthly_min'])]
df_filtered['SSM315'] = [((abs(x-y)*z)+component3.min()) for x, y, z in zip(df_filtered['SSM'], df_filtered['SSM_monthly_min'], df_filtered['sensor_slope_monthly'])]

df_filtered

start = '2024-06-15'
end = '2024-09-10'

df_crns_w = df_crns.loc[((df_crns['date'] >= start) & (df_crns['date'] <= end))]
df_crns_d = pd.concat([df_crns.loc[(df_crns['date'] <= start)], df_crns[(df_crns['date'] >= end)]])

df_filtered_w = df_filtered.loc[((df_filtered['date'] >= start) & (df_filtered['date'] <= end))]
df_filtered_d = pd.concat([df_filtered.loc[(df_filtered['date'] <= start)], df_filtered[(df_filtered['date'] >= end)]])

component2= 'SSM210'
component3= 'SSM310'

slope_w = (df_crns_w['SWC_CRNS'].max()-df_crns_w['SWC_CRNS'].min())/(df_filtered_w[component2].max()-df_filtered_w[component2].min())
slope_d = (df_crns_d['SWC_CRNS'].max()-df_crns_d['SWC_CRNS'].min())/(df_filtered_d[component2].max()-df_filtered_d[component2].min())
slope_w2 = (df_crns_w['SWC_CRNS'].max()-df_crns_w['SWC_CRNS'].min())/(df_filtered_w['SSM'].max()-df_filtered_w['SSM'].min())
slope_d2 = (df_crns_d['SWC_CRNS'].max()-df_crns_d['SWC_CRNS'].min())/(df_filtered_d['SSM'].max()-df_filtered_d['SSM'].min())
slope_sensors_w = (df_crns_w['AVG_of_three_sensors_10cm'].max()-df_crns_w['AVG_of_three_sensors_10cm'].min())/(df_filtered_w[component3].max()-df_filtered_w[component3].min())
slope_sensors_d = (df_crns_d['AVG_of_three_sensors_10cm'].max()-df_crns_d['AVG_of_three_sensors_10cm'].min())/(df_filtered_d[component3].max()-df_filtered_d[component3].min())
slope_sensors_w2 = (df_crns_w['AVG_of_three_sensors_10cm'].max()-df_crns_w['AVG_of_three_sensors_10cm'].min())/(df_filtered_w['SSM'].max()-df_filtered_w['SSM'].min())
slope_sensors_d2 = (df_crns_d['AVG_of_three_sensors_10cm'].max()-df_crns_d['AVG_of_three_sensors_10cm'].min())/(df_filtered_d['SSM'].max()-df_filtered_d['SSM'].min())

df_filtered_w['SSM2_wd'] = (abs(df_filtered_w[component2]-df_filtered_w[component2].min())*slope_w)+df_crns_w['SWC_CRNS'].min()
df_filtered_d['SSM2_wd'] = (abs(df_filtered_d[component2]-df_filtered_d[component2].min())*slope_d)+df_crns_d['SWC_CRNS'].min()
df_filtered_w['SSM3_wd'] = (abs(df_filtered_w[component3]-df_filtered_w[component3].min())*slope_sensors_w)+df_crns_w['AVG_of_three_sensors_10cm'].min()
df_filtered_d['SSM3_wd'] = (abs(df_filtered_d[component3]-df_filtered_d[component3].min())*slope_sensors_d)+df_crns_d['AVG_of_three_sensors_10cm'].min()

df_filtered_w['SSM2_wd2'] = (abs(df_filtered_w['SSM']-df_filtered_w['SSM'].min())*slope_w2)+df_crns_w['SWC_CRNS'].min()
df_filtered_d['SSM2_wd2'] = (abs(df_filtered_d['SSM']-df_filtered_d['SSM'].min())*slope_d2)+df_crns_d['SWC_CRNS'].min()
df_filtered_w['SSM3_wd2'] = (abs(df_filtered_w['SSM']-df_filtered_w['SSM'].min())*slope_sensors_w2)+df_crns_w['AVG_of_three_sensors_10cm'].min()
df_filtered_d['SSM3_wd2'] = (abs(df_filtered_d['SSM']-df_filtered_d['SSM'].min())*slope_sensors_d2)+df_crns_d['AVG_of_three_sensors_10cm'].min()


df_filtered = pd.concat([df_filtered_w, df_filtered_d])
df_filtered.sort_values(by='date')

# Time series of ndvi based on Landsat9
L9 = ee.ImageCollection("LANDSAT/LC09/C02/T1_TOA")

def addNDVI(image):
  nir = image.select('B5')
  red = image.select('B4')
  ndvi = ((nir.subtract(red)).divide(nir.add(red))).rename('NDVI')
  return image.addBands(ndvi)

L9 = L9.filterMetadata('CLOUD_COVER', 'less_Than', 50).map(addNDVI)


ts_l9 = L9.select('NDVI').filterDate(df_crns['date'].min(), df_crns['date'].max()).getRegion(AOI, 10).getInfo()
df_l9 = ee_array_to_df(ts_l9, ['NDVI'])

df_l9.sort_values(by='date')
agg_functions = {'date': 'first', 'NDVI': 'mean'}
df_l9r = df_l9.groupby((df_l9['date'])).aggregate(agg_functions)

# Timeseries of ndvi based on MODIS
ts_ndvi = ndvi.select('NDVI').filterDate(df_crns['date'].min(), df_crns['date'].max()).getRegion(AOI, 10).getInfo()
df_ndvi = ee_array_to_df(ts_ndvi, ['NDVI'])

df_ndvi['NDVI'] = df_ndvi['NDVI']*0.0001
df_ndvi.sort_values(by='date')
agg_functions = {'date': 'first', 'NDVI': 'mean'}
df_ndvir = df_ndvi.groupby((df_ndvi['date'])).aggregate(agg_functions)

# Time series of ndvi based on Landsat8
L8_TOA = ee.ImageCollection("LANDSAT/LC08/C02/T1_TOA")

def addNDVI_TOA(image):
  nir = image.select('B5')
  red = image.select('B4')
  ndvi = ((nir.subtract(red)).divide(nir.add(red))).rename('NDVI')
  return image.addBands(ndvi)

L8_TOA = L8_TOA.filterMetadata('CLOUD_COVER', 'less_Than', 50).map(addNDVI_TOA)

ts_l8 = L8_TOA.select('NDVI').filterDate(df_crns['date'].min(), df_crns['date'].max()).getRegion(AOI, 10).getInfo()
df_l8 = ee_array_to_df(ts_l8, ['NDVI'])

df_l8.sort_values(by='date')
agg_functions = {'date': 'first', 'NDVI': 'mean'}
df_l8r = df_l8.groupby((df_l8['date'])).aggregate(agg_functions)

# combining all NDVI datasets into one
frames = [df_l8r, df_l9r, df_ndvir]
df_merged_ndvi_filtered = pd.concat(frames)

df_merged_ndvi_filtered = df_merged_ndvi_filtered.loc[(df_merged_ndvi_filtered['date'] > pd.to_datetime(df_crns['date'].min()))]

length = len(df_merged_ndvi_filtered)
array = []
for i in range(length):
  array.append(i)

df_merged_ndvi_filtered['index'] = array
df_merged_ndvi_filtered = df_merged_ndvi_filtered.set_index(['index'])

df_merged_ndvi_filtered = df_merged_ndvi_filtered.groupby(pd.Grouper(key='date', freq='96h')).mean().dropna()
df_merged_ndvi_filtered['date'] = df_merged_ndvi_filtered.index

length = len(df_merged_ndvi_filtered)
array = []
for i in range(length):
  array.append(i)

df_merged_ndvi_filtered['index'] = array
df_merged_ndvi_filtered = df_merged_ndvi_filtered.set_index(['index'])
df_merged_ndvi_filtered.describe()

m_ndvi = geemap.Map()
m_ndvi.addLayer(L8_TOA.select('NDVI'), {
    'min': 0,
    'max': 1,
    'palette': ['ff0000', 'ffff00', '008000']
}, 'L8_TOA')

m_ndvi.addLayer(L9.select('NDVI'), {
    'min': 0,
    'max': 1,
    'palette': ['ff0000', 'ffff00', '008000']
}, 'L9')

m_ndvi.addLayer(ndvi.select('NDVI').map(lambda image: image.multiply(0.0001)), {
    'min': 0,
    'max': 1,
    'palette': ['ff0000', 'ffff00', '008000']
}, 'ndvi')

m_ndvi.center_object(AOI, 11)
m_ndvi

# when using premade LAI satellite imagery I get NaN results,
# possibly due to the plot being located within city boundaries
# therefor better to create own LAI based on 'raw' MODIS footage

def SR_LAI(image):

  red = image.select('sur_refl_b01').multiply(0.0001)
  nir = image.select('sur_refl_b02').multiply(0.0001)
  blue = image.select('sur_refl_b03').multiply(0.0001)

  # General formula: 2.5 * (NIR - RED) / ((NIR + 6*RED - 7.5*BLUE) + 1)
  # EVI should be between -1 to 1, for healthy vegetation this is 0.2-0.8
  EVI = ee.Image(2.5).multiply(nir.subtract(red)) \
  .divide((nir.add((ee.Image(6)).multiply(red)).subtract((ee.Image(7.5).multiply(blue)))).add(ee.Image(1)))

  # LAI should be between 0 and 3.5, although values can go much higher
  LAI = (ee.Image(3.618).multiply(ee.Image(EVI)).subtract(ee.Image(0.118))).rename('LAI')
  return image.addBands(LAI)

# add LAI band to MODIS collection and turn into dataframe
MOD_LAI = ee.ImageCollection("MODIS/061/MOD09A1") \
  .filterDate(df_filtered['date'].min(), df_filtered['date'].max()).map(SR_LAI)

ts_lai = MOD_LAI.select('LAI').getRegion(AOI, 100).getInfo()

df_lai = ee_array_to_df(ts_lai, ['LAI'])
df_lai

# Remove duplicate dates from dataframe
# instead of multiple measurements per day, only an average per day will be allowed
agg_functions = {'date': 'first', 'SSM2': 'mean','SSM22': 'mean','SSM23': 'mean','SSM24': 'mean', \
                 'SSM210': 'mean','SSM211': 'mean','SSM215': 'mean', 'SSM':'mean', \
                 'SSM3': 'mean','SSM32': 'mean','SSM33': 'mean','SSM34': 'mean', \
                 'SSM310': 'mean','SSM311': 'mean','SSM315': 'mean',\
                 'SSM2_wd': 'mean','SSM2_wd2': 'mean', 'SSM3_wd': 'mean','SSM3_wd2': 'mean', 'SSM_monthly': 'mean', 'masked_VV':'mean', 'adjusted_VV':'mean'}
df_filtered = df_filtered.groupby((df_filtered['date'])).aggregate(agg_functions)

length = len(df_filtered)
array = []
for i in range(length):
  array.append(i)

df_filtered['index'] = array
df_filtered = df_filtered.set_index(['index'])
df_filtered

#define subplots
fig,ax = plt.subplots(figsize=(20, 6))

datay1 = df_crns['SWC_CRNS']
datax1 = df_crns['date']

ndvi1 = df_l8r['NDVI']
date1 = df_l8r['date']
ndvi2 = df_l9r['NDVI']
date2 = df_l9r['date']
ndvi3 = df_ndvir['NDVI']
date3 = df_ndvir['date']
ndvi4 = df_merged_ndvi_filtered['NDVI']
date4 = df_merged_ndvi_filtered['date']

ax2 = ax.twinx()

p1 = ax.plot(datax1, datay1, label="CRNS measurements")
p2 = ax2.plot(date1, ndvi1, 's--y',c= 'grey', label="NDVI (Landsat 8)")
p3 = ax2.plot(date2, ndvi2, '>--y',c='grey', label="NDVI (Landsat 9)")
p4 = ax2.plot(date3, ndvi3, 'o--y', c='grey', label="NDVI (MODIS)")
p5 = ax2.plot(date4, ndvi4, 'o-y', c='black', label="Combined NDVI")

lns = p1+p2+p3+p4+p5
labels = [l.get_label() for l in lns]
plt.legend(lns, labels, loc="upper left")

ax.set_xlabel('Date')
ax.set_ylabel('Soil moisture percentage')
ax2.set_ylabel('NDVI')

plt.show()

# Match specific datasets for comparison. Matching too many at once will lead to loss of information
df_match1 = pd.merge_asof(df_filtered, df_crns, on="date", direction="nearest")
df_match2 = pd.merge_asof(df_match1, df_merged_ndvi_filtered, on='date', direction='nearest')
df_match3 = pd.merge_asof(df_match2, df_lai.sort_values(by='date'), on='date', direction='nearest')

df_matched = df_match3
# match soil moisture predictions to CRNS data
df_matched_crns = df_matched[['date', 'adjusted_VV', 'SSM', 'SSM2', 'SSM22','SSM23','SSM24','SSM210','SSM211','SSM215', 'SSM2_wd','SSM2_wd2', 'SWC_CRNS', 'NDVI', 'LAI']].dropna()
# match soil moisture predictions to point sensor data
df_matched_sensors = df_matched[['date', 'adjusted_VV', 'SSM', 'SSM3','SSM32','SSM33','SSM34','SSM310','SSM311','SSM315', 'SSM3_wd','SSM3_wd2', 'NDVI', 'LAI', 'AVG_of_three_sensors_10cm']].dropna()

df_matched_ndvi = pd.merge_asof(df_merged_ndvi_filtered, df_match1, on='date', direction='nearest')

df_matched_ndvi_sensors = df_matched_ndvi[['date', 'adjusted_VV', 'SSM', 'SSM2', 'SSM3', 'SWC_CRNS', 'NDVI', 'AVG_of_three_sensors_10cm']].dropna()
df_matched_ndvi_crns = df_matched_ndvi[['date', 'adjusted_VV', 'SSM', 'SSM2', 'SSM3', 'SWC_CRNS', 'NDVI']].dropna()


df_matched_lai = pd.merge_asof(df_lai.sort_values(by='date'), df_match1, on='date', direction='nearest')

df_matched_lai_sensors = df_matched_lai[['date', 'adjusted_VV', 'SSM', 'SSM2', 'SSM3', 'SWC_CRNS', 'LAI', 'AVG_of_three_sensors_10cm']].dropna().drop_duplicates()
df_matched_lai_crns = df_matched_lai[['date', 'adjusted_VV', 'SSM', 'SSM2', 'SSM3', 'SWC_CRNS', 'LAI']].dropna().drop_duplicates()

df_matched_crns_sensor = df_crns.dropna()

#define subplots
fig,ax = plt.subplots(figsize=(20, 6))

datay1 = df_crns['SWC_CRNS']
datax1 = df_crns['date']
datay2 = df_crns['AVG_of_three_sensors_10cm']
datay3 = df_filtered['SSM2']
datax3 = df_filtered['date']
datay4 = df_matched_crns['SSM24']
datax4 = df_matched_crns['date']
ax2 = ax.twinx()

ax.plot(datax1, datay1)
ax.plot(datax4, datay4, 'o-y', c='black')


plt.show()

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score
from sklearn.metrics import root_mean_squared_error

X = df_matched_crns['SSM24']
y = df_matched_crns['SWC_CRNS']
X = df_matched_sensors['SSM34']
y = df_matched_sensors['AVG_of_three_sensors_10cm']

df1 = df_matched_lai_sensors['LAI']
df2 = df_matched_lai_sensors['AVG_of_three_sensors_10cm']
df1 = df_matched_sensors['SSM310']
df2 = df_matched_sensors['AVG_of_three_sensors_10cm']
df1 = df_matched_ndvi_sensors['NDVI']
df2 = df_matched_ndvi_sensors['AVG_of_three_sensors_10cm']

scaler = MinMaxScaler(feature_range=(0, 1))
X = scaler.fit_transform(df1.to_numpy().reshape(-1, 1)).ravel()
y = scaler.fit_transform(df2.to_numpy().reshape(-1, 1)).ravel()


degree = 1
model = np.poly1d(np.polyfit(X, y, degree))
def polyfit(x, y, degree):
    results = {}
    coeffs = np.polyfit(x, y, degree)
    p = np.poly1d(coeffs)
    #calculate r-squared
    yhat = p(x)
    ybar = np.sum(y)/len(y)
    ssreg = np.sum((yhat-ybar)**2)
    sstot = np.sum((y - ybar)**2)
    results['r_squared'] = ssreg / sstot

    return results
print(polyfit(X, y, degree))
def rmse(predictions, targets):
    return np.sqrt(((predictions - targets) ** 2).mean())
rmse_val = rmse(np.array(X), np.array(y))
print("rms error is: " + str(rmse_val))

print(model)

#add fitted polynomial line to scatterplot
fig,ax = plt.subplots(figsize=(6, 6))
polyline = np.linspace(X.min(), X.max())
plt.scatter(X, y, s=2)
plt.plot(polyline, model(polyline))

from scipy.stats import linregress
slope, intercept, r_value, p_value, std_err = linregress(X, y)
ax.annotate(f'R-squared = {r_value**2:.8f}',(30,61))
ax.annotate(f'RMSE = {rmse_val:.8f}',(30,59))
plt.xlabel('X')
plt.ylabel('Y')
plt.show()

# Calculate statistics for evaluation of different SSM prediction methods

from scipy.spatial.distance import correlation
from scipy.stats import spearmanr

def statistics(true_value, predicted_values):
  records = []
  for i in predicted_values:
    X = predicted_values[i]
    y = true_value
    nameX = str(i)
    namey = str(df)

    rho, p = spearmanr(X, y)
    corelation = correlation(X, y)

    x_mean = (sum(X))/float(len(X))
    y_mean = (sum(y))/float(len(y))

    top = sum((X-x_mean)*(y-y_mean))
    bottom1 = sum((X-x_mean)**2)
    bottom2 = sum((y-y_mean)**2)
    bottom = (bottom1*bottom2)**(1/2)
    r = top/bottom
    Pr2 = r**2
    r2   = r2_score(y.values, X.values)
    rmse = root_mean_squared_error(y.values, X.values)

    records.append((nameX, namey, Pr2, r2, rmse, rho, corelation))

  cols = ["X", "y", 'Pr2', "r2", "rmse", "rho", "corelation"]
  return (
      pd.DataFrame.from_records(records, columns=cols)
  )

# print statistics for comapring SSM retrieval with in-field CRNS data
statistics(df_matched_crns['SWC_CRNS'], df_matched_crns[['SSM','SSM2', 'SSM22','SSM23', 'SSM24', 'SSM210', 'SSM211', 'SSM215', 'SSM2_wd', 'SSM2_wd2']])

# print statistics for comapring SSM retrieval with in-field point sensor data
statistics(df_matched_sensors['AVG_of_three_sensors_10cm'], df_matched_sensors[['SSM','SSM3', 'SSM32','SSM33', 'SSM34', 'SSM310', 'SSM311', 'SSM315', 'SSM3_wd', 'SSM3_wd2']])

raise SystemExit("Stop right there!")

# After this part some attempts were done on the adjustment of SSM based on vegetation data (through NDVI and LAI)
# This however was done on a rudimentary level do to time restrictions and did not provide any satisfactory results

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_squared_error


# ---------------------------------------------------------------
# 1️⃣  (i, j, k) generator on a 0.01 grid with i + j + k = 1
# ---------------------------------------------------------------
def triples_sum_to_one(step=0.01):
    scale = round(1 / step)                       # 0.01 → 100
    for i_int in range(scale + 1):
        for j_int in range(scale + 1):
            k_int = scale - i_int - j_int         # keep the sum = 1
            if 0 <= k_int <= scale:
                yield i_int / scale, j_int / scale, k_int / scale


# ---------------------------------------------------------------
# 2️⃣  helper: scale a Series to (–1, 1)
# ---------------------------------------------------------------
def scale_minus1_to_1(series):
    scaler = MinMaxScaler(feature_range=(-1, 1))
    return scaler.fit_transform(series.to_numpy().reshape(-1, 1)).ravel()


# ---------------------------------------------------------------
# 3️⃣  main evaluation routine  (-1,1 scaling  ➜  new coefficients)
# ---------------------------------------------------------------
def evaluate_triples(df_matched_sensors, df_X, step=0.01):
    # a) individually scale the three raw variables to (-1, 1)
    SSM_scaled  = scale_minus1_to_1(df_matched_sensors["SSM310"])
    NDVI_scaled = scale_minus1_to_1(df_matched_sensors["NDVI"])
    LAI_scaled  = scale_minus1_to_1(df_matched_sensors["LAI"])

    y_true = df_X.to_numpy()
    y_min, y_max = y_true.min(), y_true.max()

    records = []
    for i, j, k in triples_sum_to_one(step):
        y_pred_raw = (
              i * ( SSM_scaled )
            + j * (-0.4609 * NDVI_scaled + 0.8327 )
            + k * (-0.4131 * LAI_scaled  + 0.7563 )
        )

        # 0.7208 x + 0.1885 #SSM310
        # 0.9039 * SSM_scaled  + 0.004006 #SSM3_wd


        # (optional) map the prediction back to the span of df_X
        y_pred_scaled = (y_pred_raw - y_pred_raw.min()) / (y_pred_raw.max() - y_pred_raw.min())
        y_pred = y_pred_scaled * (y_max - y_min) + y_min

        mse  = mean_squared_error(y_true, y_pred)   # works on any scikit-learn version
        rmse = np.sqrt(mse)
        r2   = r2_score(y_true, y_pred)

        records.append((i, j, k, r2, rmse))

    cols = ["i", "j", "k", "r_squared", "rmse"]
    return (
        pd.DataFrame.from_records(records, columns=cols)
          .sort_values("i")
          .reset_index(drop=True)
    )

results = evaluate_triples(df_matched_sensors, df_matched_sensors['AVG_of_three_sensors_10cm'])
with pd.option_context('display.max_rows', None,
                       'display.max_columns', None,
                       'display.precision', 3,
                       ):
    print(results)

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_squared_error

# ---------------------------------------------------------------
# 1️⃣  (i, j, k) generator on a 0.01 grid with i + j + k = 1
# ---------------------------------------------------------------
def triples_sum_to_one(step=0.01):
    scale = round(1 / step)                       # 0.01 → 100
    for i_int in range(scale + 1):
        for j_int in range(scale + 1):
            k_int = scale - i_int - j_int         # keep the sum = 1
            if 0 <= k_int <= scale:
                yield i_int / scale, j_int / scale, k_int / scale


# ---------------------------------------------------------------
# 2️⃣  helper: scale a Series to (–1, 1)
# ---------------------------------------------------------------
def scale_minus1_to_1(series):
    scaler = MinMaxScaler(feature_range=(-1, 1))
    return scaler.fit_transform(series.to_numpy().reshape(-1, 1)).ravel()


# ---------------------------------------------------------------
# 3️⃣  main evaluation routine  (-1,1 scaling  ➜  new coefficients)
# ---------------------------------------------------------------
def evaluate_triples(df_matched_crns, df_X, step=0.01):
    # a) individually scale the three raw variables to (-1, 1)
    SSM_scaled  = scale_minus1_to_1(df_matched_crns["SSM2_wd"])
    NDVI_scaled = scale_minus1_to_1(df_matched_crns["NDVI"])
    LAI_scaled  = scale_minus1_to_1(df_matched_crns["LAI"])

    y_true = df_X.to_numpy()
    y_min, y_max = y_true.min(), y_true.max()

    records = []
    for i, j, k in triples_sum_to_one(step):
        y_pred_raw = (
              i * (0.688 * SSM_scaled + 0.1684)
              + j * (-0.2587 * NDVI_scaled   + 0.7527)
              + k * (-0.3811  * LAI_scaled    + 0.6832)
        )

        # (optional) map the prediction back to the span of df_X
        y_pred_scaled = (y_pred_raw - y_pred_raw.min()) / (y_pred_raw.max() - y_pred_raw.min())
        y_pred = y_pred_scaled * (y_max - y_min) + y_min

        mse  = mean_squared_error(y_true, y_pred)   # works on any scikit-learn version
        rmse = np.sqrt(mse)
        r2   = r2_score(y_true, y_pred)

        records.append((i, j, k, r2, rmse))

    cols = ["i", "j", "k", "r_squared", "rmse"]
    return (
        pd.DataFrame.from_records(records, columns=cols)
          .sort_values("r_squared")
          .reset_index(drop=True)
    )

results = evaluate_triples(df_matched_crns, df_matched_crns['SWC_CRNS'])
with pd.option_context('display.max_rows', None,
                       'display.max_columns', None,
                       'display.precision', 3,
                       ):
    print(results)

df_matched_sensors['model310'] = \
  [((0.85*(x)) + \
  (0.15*(-0.4609*y + 0.8327)) + \
  (0*(-0.5279*z + 0.8791))) \
  for x,y,z in zip(df_matched_sensors['SSM310'], df_matched_sensors['NDVI'], df_matched_sensors['LAI'])]

X = df_matched_sensors['SSM3_wd']
y = df_matched_sensors['AVG_of_three_sensors_10cm']
X = df_matched_sensors['model310']
y = df_matched_sensors['AVG_of_three_sensors_10cm']

degree = 1
model = np.poly1d(np.polyfit(X, y, degree))
print(model)

def polyfit(x, y, degree):
    results = {}

    coeffs = np.polyfit(x, y, degree)
    p = np.poly1d(coeffs)
    #calculate r-squared
    yhat = p(x)
    ybar = np.sum(y)/len(y)
    ssreg = np.sum((yhat-ybar)**2)
    sstot = np.sum((y - ybar)**2)
    results['r_squared'] = ssreg / sstot

    return results
print(polyfit(X, y, degree))

#add fitted polynomial line to scatterplot
fig,ax = plt.subplots(figsize=(8, 8))
polyline = np.linspace(X.min(), X.max())
plt.scatter(X, y)
plt.plot(polyline, model(polyline))
def rmse(predictions, targets):
    return np.sqrt(((predictions - targets) ** 2).mean())
rmse_val = rmse(np.array(X), np.array(y))
print("rms error is: " + str(rmse_val))
slope, intercept, r_value, p_value, std_err = linregress(X, y)
plt.annotate(f'R-squared = {r_value**2:.8f}',(30,58))
plt.annotate(f'RMSE = {rmse_val:.8f}',(30,56))
plt.xlabel('Soil moisture value by CRNS')
plt.ylabel('Soil moisture value by point sensors')
plt.show()